name: "SLURM"   # MUST BE "SLURM"

# Required
partition: "single"
job-name: "ALRProject"    # this will be the experiments name in slurm

# Required - Cluster Specific
num_parallel_jobs: 99
ntasks: 1
cpus-per-task: 16
mem-per-cpu: 8000
time: 1440  # in minutes

sbatch_args: # Dictionary of SBATCH keywords and arguments
  distribution: cyclic  # To have repetitions of the same exp be distributed to different nodes

slurm_log: "/home/kit/anthropomatik/ca9551/irl_eim/I2RL/experiments/slurmlog" # optional. dir in which slurm output and error logs will be saved.
---

name: "DEFAULT"   # MUST BE DEFAULT
import_path: "default.yaml"

optuna:
    loss_key: "test_accuracy"

compute:
  preprocessing: False
  retrain: False # Start training a new model if True. Continue training the most recent version of the model if False.

params:
  retrain: False
  task:
    task: mesh
    dataset: cylinder
    rollout_split: valid
    batch_size: 1
    epochs: 1
    trajectories: 1
    mode: all
    prefetch_factor: 1
    test:
      trajectories: 1 # Number of trajectories that are going to be used to compute the validation loss during evaluation
      rollouts: 1 # Number of trajectories that are going to be used to compute the rollout loss during evaluation
      n_step_rollouts: 1 # Number of trajectories that are going to be used to compute the n_step_loss during evaluation
      n_steps: 60
    validation:
      trajectories: 1 # Number of trajectories that are going to be used to compute the validation loss during training
      rollouts: 1 # Number of trajectories that are going to be used to compute the rollout loss during training
  model:
    field: world_pos
    history: True
    size: 4
    noise: 0.02
    gamma: 0.9
    scheduler_epoch: 10
    aggregation: pna
    learning_rate: 1.0e-4
    message_passing_steps: 1
    rmp:
      num_clusters: 15
      frequency: 1
      clustering: none
      connector: none
      fully_connect: False
      intra_cluster_sampling:
        enabled: False
        alpha: 0.1
        spotter_threshold: 0
      hdbscan:
        max_cluster_size: 50
        min_cluster_size: 20
        min_samples: 1
        spotter_threshold: 0.9
    graph_balancer:
      algorithm: none
      frequency: 1
      remove_edges: True
      ricci:
        loops: 150
        tau: 150
      random:
        edge_amount: 100
  logging:
    wandb_mode: online
  random_seed: 0

---
name: optuna_exp
repetitions: 500
params:
  recording:
    plot_frequency: 100  # record every 100th iteration

optuna_hps:  # we automatically switch to optuna experiments when "optuna_hps" is provided for the experiment
  # again two entries, first specifies which "suggest" function to call, second the arguments for that function
  algorithm:
    network:
      feedforward:
        num_layers: ["int", [2, 4]]
        log_max_neurons: ["int", [4,6]]
        network_shape: ["categorical", [["=","<", ">", "<>", "><"]]]

---
name: local_exp
repetitions: 1
iterations: 11
params:
  recording:
    wandb: false
    plot_frequency: 10
