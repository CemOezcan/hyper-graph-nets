name: "SLURM"   # MUST BE "SLURM"

# Required
partition: "single"
job-name: "ALRProject"    # this will be the experiments name in slurm

# Required - Cluster Specific
num_parallel_jobs: 99
ntasks: 1
cpus-per-task: 16
mem-per-cpu: 8000
time: 1440  # in minutes

sbatch_args: # Dictionary of SBATCH keywords and arguments
  distribution: cyclic  # To have repetitions of the same exp be distributed to different nodes

slurm_log: "/home/kit/anthropomatik/ca9551/irl_eim/I2RL/experiments/slurmlog" # optional. dir in which slurm output and error logs will be saved.
---

name: "DEFAULT"   # MUST BE DEFAULT
import_path: "default.yaml"

optuna:
    loss_key: "test_accuracy"

params:
  task:
    dataset: flag_minimal
    rollout_split: valid
    batch_size: 1 # TODO: Batch size != 1 raises an error
    epochs: 100
    trajectories: 11
    rollouts: 1
    mode: all
    prefetch_factor: 2
  model:
    field: world_pos
    history: True
    size: 3
    noise: 0.003
    gamma: 0.1
    aggregation: pna
    learning_rate: 1.0e-4
    scheduler_learning_rate:  1.0e-4
    message_passing_steps: 1
    attention: False
    load: None
    stochastic_message_passing_used: False
    rmp:
      ricci: True
      clustering: hdbscan
      connector: hierarchical

---
name: optuna_exp
repetitions: 500
params:
  recording:
    plot_frequency: 100  # record every 100th iteration

optuna_hps:  # we automatically switch to optuna experiments when "optuna_hps" is provided for the experiment
  # again two entries, first specifies which "suggest" function to call, second the arguments for that function
  algorithm:
    network:
      feedforward:
        num_layers: ["int", [2, 4]]
        log_max_neurons: ["int", [4,6]]
        network_shape: ["categorical", [["=","<", ">", "<>", "><"]]]

---
name: local_exp
repetitions: 1
iterations: 11
params:
  recording:
    wandb: false
    plot_frequency: 10
