name: "SLURM"   # MUST BE "SLURM"

# Required
partition: "single"
job-name: "ALRProject"    # this will be the experiments name in slurm

# Required - Cluster Specific
num_parallel_jobs: 99
ntasks: 1
cpus-per-task: 16
mem-per-cpu: 8000
time: 1440  # in minutes

sbatch_args: # Dictionary of SBATCH keywords and arguments
  distribution: cyclic  # To have repetitions of the same exp be distributed to different nodes

slurm_log: "/home/kit/anthropomatik/ca9551/irl_eim/I2RL/experiments/slurmlog" # optional. dir in which slurm output and error logs will be saved.
---

name: "DEFAULT"   # MUST BE DEFAULT
import_path: "default.yaml"

optuna:
    loss_key: "test_accuracy"

compute:
  preprocessing: False
  retrain: True # Start training a new model if True. Continue training the most recent version of the model if False.

params:
  task:
    dataset: flag_simple
    rollout_split: valid
    batch_size: 21
    epochs: 15
    trajectories: 1000
    mode: all
    prefetch_factor: 5
    test:
      trajectories: 100 # Number of trajectories that are going to be used to compute the validation loss during evaluation
      rollouts: 10 # Number of trajectories that are going to be used to compute the rollout loss during evaluation
      n_step_rollouts: 1 # Number of trajectories that are going to be used to compute the n_step_loss during evaluation
      n_steps: 60
    validation:
      trajectories: 20 # Number of trajectories that are going to be used to compute the validation loss during training
      rollouts: 5 # Number of trajectories that are going to be used to compute the rollout loss during training
  model:
    field: world_pos
    history: True
    size: 3
    noise: 0.003
    gamma: 0.9
    scheduler_epoch: 10
    aggregation: pna
    learning_rate: 5.0e-5
    message_passing_steps: 5
    rmp:
      num_clusters: 15
      frequency: 399
      clustering: gmm
      connector: hierarchical
      intra_cluster_sampling:
        enabled: False
        alpha: 0.1
        spotter_threshold: 0
      hdbscan:
        max_cluster_size: 50
        min_cluster_size: 20
        min_samples: 1
        spotter_threshold: 0.9
    graph_balancer:
      algorithm: none
      frequency: 1
      ricci:
        loops: 150
        tau: 150
      random:
        edge_amount: 100
  logging:
    wandb_mode: run
  random_seed: 0

---
name: optuna_exp
repetitions: 500
params:
  recording:
    plot_frequency: 100  # record every 100th iteration

optuna_hps:  # we automatically switch to optuna experiments when "optuna_hps" is provided for the experiment
  # again two entries, first specifies which "suggest" function to call, second the arguments for that function
  algorithm:
    network:
      feedforward:
        num_layers: ["int", [2, 4]]
        log_max_neurons: ["int", [4,6]]
        network_shape: ["categorical", [["=","<", ">", "<>", "><"]]]

---
name: local_exp
repetitions: 1
iterations: 11
params:
  recording:
    wandb: false
    plot_frequency: 10
